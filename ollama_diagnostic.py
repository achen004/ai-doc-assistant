"""Diagnostic script to check Ollama setup and connection."""

#In WSL /usr/local/bin create setup_port_forward.sh
#echo "/usr/local/bin/setup_port_forward.sh" >> ~/.bashrc to autorun on WSl startup
#netstat -tulpn | grep 11434
#sudo lsof -i :11434
#sudo kill -9 [PID]
#netsh interface portproxy delete v4tov4 listenport=PORT_NUMBER listenaddress=127.0.0.1 

import requests
import json
from langchain.llms import Ollama
import time

def check_ollama_service():
    """Check if Ollama service is running."""
    print("üîç Checking Ollama service...")
    
    try:
        response = requests.get("http://127.0.0.1:11434/api/tags", timeout=10)
        if response.status_code == 200:
            print("‚úÖ Ollama service is running")
            
            data = response.json()
            models = data.get('models', [])
            print(f"üì¶ Available models: {[model['name'] for model in models]}")
            
            return True, models
        else:
            print(f"‚ùå Ollama service responded with status code: {response.status_code}")
            return False, []
            
    except requests.exceptions.ConnectionError:
        print("‚ùå Cannot connect to Ollama service at http://127.0.0.1:11434")
        print("üí° Make sure Ollama is running: ollama serve")
        return False, []
    
    except requests.exceptions.Timeout:
        print("‚ùå Ollama service is not responding (timeout)")
        return False, []
    
    except Exception as e:
        print(f"‚ùå Unexpected error checking Ollama: {str(e)}")
        return False, []


def test_model_availability(model_name="mistral"):
    """Test if a specific model is available."""
    print(f"\nüîç Testing model availability: {model_name}")
    
    try:
        # Try to get model info
        response = requests.get(f"http://127.0.0.1:11434/api/show", 
                              json={"name": model_name}, 
                              timeout=10)
        
        if response.status_code == 200:
            print(f"‚úÖ Model '{model_name}' is available")
            return True
        else:
            print(f"‚ùå Model '{model_name}' not found")
            print(f"üí° Try running: ollama pull {model_name}")
            return False
            
    except Exception as e:
        print(f"‚ùå Error checking model: {str(e)}")
        return False


def test_langchain_ollama(model_name="mistral"):
    """Test LangChain Ollama integration."""
    print(f"\nüîç Testing LangChain Ollama integration with {model_name}...")
    
    try:
        # Create Ollama instance
        llm = Ollama(
            base_url="http://127.0.0.1:11434",
            model=model_name,
            timeout=30  # Increase timeout
        )
        
        # Test simple query
        print("üß™ Testing simple query...")
        start_time = time.time()
        response = llm("Hello, how are you? Please respond briefly.")
        end_time = time.time()
        
        print(f"‚úÖ LangChain integration working!")
        print(f"‚è±Ô∏è  Response time: {end_time - start_time:.2f} seconds")
        print(f"üìù Response: {response[:100]}{'...' if len(response) > 100 else ''}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå LangChain Ollama test failed: {str(e)}")
        
        # Check if it's a connection issue
        if "Connection" in str(e) or "connection" in str(e).lower():
            print("üí° This looks like a connection issue. Check if:")
            print("   - Ollama is running: ollama serve")
            print("   - No firewall blocking port 11434")
            print("   - Model is loaded: ollama run mistral")
        
        return False


def test_model_performance(model_name="mistral"):
    """Test model performance with longer query."""
    print(f"\nüîç Testing model performance with longer query...")
    
    try:
        llm = Ollama(
            base_url="http://127.0.0.1:11434",
            model=model_name,
            timeout=60  # Longer timeout for complex queries
        )
        
        test_query = """
        Based on this context: "The weather is sunny and warm today. 
        People are enjoying outdoor activities in the park."
        
        Question: What is the weather like?
        """
        
        print("üß™ Testing longer query with context...")
        start_time = time.time()
        response = llm(test_query)
        end_time = time.time()
        
        print(f"‚úÖ Performance test passed!")
        print(f"‚è±Ô∏è  Response time: {end_time - start_time:.2f} seconds")
        print(f"üìù Response: {response}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Performance test failed: {str(e)}")
        return False


def main():
    """Run all diagnostic tests."""
    print("üöÄ Ollama Diagnostic Script")
    print("=" * 50)
    
    # Step 1: Check service
    service_ok, models = check_ollama_service()
    
    if not service_ok:
        print("\n‚ùå Cannot proceed - Ollama service is not running")
        print("\nüîß To fix:")
        print("1. Install Ollama from https://ollama.ai")
        print("2. Start the service: ollama serve")
        print("3. Pull a model: ollama pull mistral")
        return
    
    # Step 2: Check model
    model_name = "mistral"
    if models:
        # Use first available model if mistral not found
        available_model_names = [model['name'].split(':')[0] for model in models]
        if model_name not in available_model_names:
            model_name = available_model_names[0]
            print(f"‚ÑπÔ∏è  Using available model: {model_name}")
    
    model_ok = test_model_availability(model_name)
    
    if not model_ok:
        print(f"\n‚ùå Model '{model_name}' not available")
        print(f"üîß To fix: ollama pull {model_name}")
        return
    
    # Step 3: Test LangChain integration
    langchain_ok = test_langchain_ollama(model_name)
    
    if not langchain_ok:
        return
    
    # Step 4: Test performance
    perf_ok = test_model_performance(model_name)
    
    # Summary
    print("\n" + "=" * 50)
    print("üìä DIAGNOSTIC SUMMARY")
    print("=" * 50)
    print(f"Ollama Service: {'‚úÖ' if service_ok else '‚ùå'}")
    print(f"Model Available: {'‚úÖ' if model_ok else '‚ùå'}")
    print(f"LangChain Integration: {'‚úÖ' if langchain_ok else '‚ùå'}")
    print(f"Performance Test: {'‚úÖ' if perf_ok else '‚ùå'}")
    
    if all([service_ok, model_ok, langchain_ok, perf_ok]):
        print("\nüéâ All tests passed! Your Ollama setup should work with the QA chain.")
    else:
        print("\n‚ö†Ô∏è  Some tests failed. Please address the issues above.")


if __name__ == "__main__":
    main()